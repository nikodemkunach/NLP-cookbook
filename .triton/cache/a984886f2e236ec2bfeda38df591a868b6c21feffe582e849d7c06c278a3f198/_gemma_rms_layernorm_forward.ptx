//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_75
.address_size 64

	// .globl	_gemma_rms_layernorm_forward
.extern .shared .align 16 .b8 global_smem[];
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};

.visible .entry _gemma_rms_layernorm_forward(
	.param .u64 _gemma_rms_layernorm_forward_param_0,
	.param .u32 _gemma_rms_layernorm_forward_param_1,
	.param .u64 _gemma_rms_layernorm_forward_param_2,
	.param .u32 _gemma_rms_layernorm_forward_param_3,
	.param .u64 _gemma_rms_layernorm_forward_param_4,
	.param .u64 _gemma_rms_layernorm_forward_param_5,
	.param .u32 _gemma_rms_layernorm_forward_param_6,
	.param .f32 _gemma_rms_layernorm_forward_param_7
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<28>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<106>;
	.reg .f32 	%f<119>;
	.reg .b64 	%rd<18>;
	.loc	1 103 0
$L__func_begin0:
	.loc	1 103 0

	ld.param.u64 	%rd8, [_gemma_rms_layernorm_forward_param_0];
	ld.param.u32 	%r52, [_gemma_rms_layernorm_forward_param_1];
$L__tmp0:
	.loc	1 114 28
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	ld.param.u64 	%rd9, [_gemma_rms_layernorm_forward_param_2];
	.loc	1 115 31
	mov.u32 	%r53, %tid.x;
	and.b32  	%r54, %r53, 31;
	ld.param.u32 	%r55, [_gemma_rms_layernorm_forward_param_3];
	ld.param.u64 	%rd10, [_gemma_rms_layernorm_forward_param_4];
	shl.b32 	%r56, %r53, 3;
	ld.param.u64 	%rd11, [_gemma_rms_layernorm_forward_param_5];
	and.b32  	%r57, %r56, 2040;
	ld.param.u32 	%r58, [_gemma_rms_layernorm_forward_param_6];
	or.b32  	%r59, %r57, 2048;
	ld.param.f32 	%f1, [_gemma_rms_layernorm_forward_param_7];
	.loc	1 116 25
	setp.lt.s32 	%p1, %r57, %r58;
	setp.lt.s32 	%p6, %r59, %r58;
	.loc	1 119 19
	mul.lo.s32 	%r60, %r1, %r55;
	.loc	1 119 9
	mul.wide.s32 	%rd12, %r60, 2;
	add.s64 	%rd13, %rd9, %rd12;
	.loc	1 122 24
	mul.wide.u32 	%rd14, %r57, 2;
	add.s64 	%rd1, %rd13, %rd14;
	add.s64 	%rd2, %rd1, 4096;
	mov.b32 	%r6, 0;
	.loc	1 122 20
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	mov.u32 %r4, 0x0;
	mov.u32 %r5, 0x0;
	@%p1 ld.global.v4.b32 { %r2, %r3, %r4, %r5 }, [ %rd1 + 0 ];
	@!%p1 mov.u32 %r2, %r6;
	@!%p1 mov.u32 %r3, %r6;
	@!%p1 mov.u32 %r4, %r6;
	@!%p1 mov.u32 %r5, %r6;
	// end inline asm
	// begin inline asm
	mov.u32 %r10, 0x0;
	mov.u32 %r11, 0x0;
	mov.u32 %r12, 0x0;
	mov.u32 %r13, 0x0;
	@%p6 ld.global.v4.b32 { %r10, %r11, %r12, %r13 }, [ %rd2 + 0 ];
	@!%p6 mov.u32 %r10, %r6;
	@!%p6 mov.u32 %r11, %r6;
	@!%p6 mov.u32 %r12, %r6;
	@!%p6 mov.u32 %r13, %r6;
	// end inline asm
	.loc	1 122 64
	mov.b32 	{%rs1, %rs2}, %r2;
	cvt.f32.f16 	%f2, %rs1;
	cvt.f32.f16 	%f3, %rs2;
	mov.b32 	{%rs3, %rs4}, %r3;
	cvt.f32.f16 	%f4, %rs4;
	cvt.f32.f16 	%f5, %rs3;
	mov.b32 	{%rs5, %rs6}, %r4;
	cvt.f32.f16 	%f6, %rs6;
	cvt.f32.f16 	%f7, %rs5;
	mov.b32 	{%rs7, %rs8}, %r5;
	cvt.f32.f16 	%f8, %rs8;
	cvt.f32.f16 	%f9, %rs7;
	mov.b32 	{%rs9, %rs10}, %r10;
	cvt.f32.f16 	%f10, %rs10;
	cvt.f32.f16 	%f11, %rs9;
	mov.b32 	{%rs11, %rs12}, %r11;
	cvt.f32.f16 	%f12, %rs12;
	cvt.f32.f16 	%f13, %rs11;
	mov.b32 	{%rs13, %rs14}, %r12;
	cvt.f32.f16 	%f14, %rs14;
	cvt.f32.f16 	%f15, %rs13;
	mov.b32 	{%rs15, %rs16}, %r13;
	cvt.f32.f16 	%f16, %rs16;
	cvt.f32.f16 	%f17, %rs15;
	.loc	1 123 24
	add.s64 	%rd3, %rd10, %rd14;
	add.s64 	%rd4, %rd3, 4096;
	.loc	1 123 20
	// begin inline asm
	mov.u32 %r18, 0x0;
	mov.u32 %r19, 0x0;
	mov.u32 %r20, 0x0;
	mov.u32 %r21, 0x0;
	@%p1 ld.global.v4.b32 { %r18, %r19, %r20, %r21 }, [ %rd3 + 0 ];
	@!%p1 mov.u32 %r18, %r6;
	@!%p1 mov.u32 %r19, %r6;
	@!%p1 mov.u32 %r20, %r6;
	@!%p1 mov.u32 %r21, %r6;
	// end inline asm
	// begin inline asm
	mov.u32 %r26, 0x0;
	mov.u32 %r27, 0x0;
	mov.u32 %r28, 0x0;
	mov.u32 %r29, 0x0;
	@%p6 ld.global.v4.b32 { %r26, %r27, %r28, %r29 }, [ %rd4 + 0 ];
	@!%p6 mov.u32 %r26, %r6;
	@!%p6 mov.u32 %r27, %r6;
	@!%p6 mov.u32 %r28, %r6;
	@!%p6 mov.u32 %r29, %r6;
	// end inline asm
	.loc	1 125 29
	mul.f32 	%f18, %f3, %f3;
$L__tmp1:
	.loc	2 256 15
	fma.rn.f32 	%f19, %f2, %f2, %f18;
	fma.rn.f32 	%f20, %f5, %f5, %f19;
	fma.rn.f32 	%f21, %f4, %f4, %f20;
	fma.rn.f32 	%f22, %f7, %f7, %f21;
	fma.rn.f32 	%f23, %f6, %f6, %f22;
	fma.rn.f32 	%f24, %f9, %f9, %f23;
	fma.rn.f32 	%f25, %f8, %f8, %f24;
	fma.rn.f32 	%f26, %f11, %f11, %f25;
	fma.rn.f32 	%f27, %f10, %f10, %f26;
	fma.rn.f32 	%f28, %f13, %f13, %f27;
	fma.rn.f32 	%f29, %f12, %f12, %f28;
	fma.rn.f32 	%f30, %f15, %f15, %f29;
	fma.rn.f32 	%f31, %f14, %f14, %f30;
	fma.rn.f32 	%f32, %f17, %f17, %f31;
	fma.rn.f32 	%f33, %f16, %f16, %f32;
	.loc	2 267 36
	mov.b32 	%r77, %f33;
	shfl.sync.bfly.b32	%r78, %r77, 16, 31, -1;
	mov.b32 	%f34, %r78;
	.loc	2 256 15
	add.f32 	%f35, %f33, %f34;
	.loc	2 267 36
	mov.b32 	%r79, %f35;
	shfl.sync.bfly.b32	%r80, %r79, 8, 31, -1;
	mov.b32 	%f36, %r80;
	.loc	2 256 15
	add.f32 	%f37, %f35, %f36;
	.loc	2 267 36
	mov.b32 	%r81, %f37;
	shfl.sync.bfly.b32	%r82, %r81, 4, 31, -1;
	mov.b32 	%f38, %r82;
	.loc	2 256 15
	add.f32 	%f39, %f37, %f38;
	.loc	2 267 36
	mov.b32 	%r83, %f39;
	shfl.sync.bfly.b32	%r84, %r83, 2, 31, -1;
	mov.b32 	%f40, %r84;
	.loc	2 256 15
	add.f32 	%f41, %f39, %f40;
	.loc	2 267 36
	mov.b32 	%r85, %f41;
	shfl.sync.bfly.b32	%r86, %r85, 1, 31, -1;
	mov.b32 	%f42, %r86;
	.loc	2 256 15
	add.f32 	%f43, %f41, %f42;
	.loc	2 267 36
	setp.eq.s32 	%p21, %r54, 0;
	shr.u32 	%r87, %r53, 3;
	and.b32  	%r88, %r87, 28;
	mov.u32 	%r89, global_smem;
	add.s32 	%r34, %r89, %r88;
	mov.b32 	%r35, %f43;
	// begin inline asm
	@%p21 st.shared.b32 [ %r34 + 0 ], %r35;
	// end inline asm
	bar.sync 	0;
	setp.lt.s32 	%p22, %r53, 8;
	shl.b32 	%r90, %r53, 2;
	add.s32 	%r37, %r89, %r90;
	// begin inline asm
	@%p22 ld.shared.b32 %r36, [ %r37 + 0 ];
	// end inline asm
	mov.b32 	%f44, %r36;
	shfl.sync.bfly.b32	%r91, %r36, 4, 31, -1;
	mov.b32 	%f45, %r91;
	.loc	2 256 15
	add.f32 	%f46, %f44, %f45;
	.loc	2 267 36
	mov.b32 	%r92, %f46;
	shfl.sync.bfly.b32	%r93, %r92, 2, 31, -1;
	mov.b32 	%f47, %r93;
	.loc	2 256 15
	add.f32 	%f48, %f46, %f47;
	.loc	2 267 36
	mov.b32 	%r94, %f48;
	shfl.sync.bfly.b32	%r95, %r94, 1, 31, -1;
	mov.b32 	%f49, %r95;
	.loc	2 256 15
	add.f32 	%f50, %f48, %f49;
	.loc	2 267 36
	and.b32  	%r96, %r53, 7;
	setp.eq.s32 	%p27, %r96, 0;
	and.pred  	%p23, %p22, %p27;
	mov.b32 	%r39, %f50;
	// begin inline asm
	@%p23 st.shared.b32 [ %r37 + 0 ], %r39;
	// end inline asm
	bar.sync 	0;
$L__tmp2:
	.loc	1 125 48
	cvt.rn.f32.s32 	%f51, %r58;
	mov.b32 	%r42, %f51;
	ld.shared.u32 	%r41, [global_smem];
	// begin inline asm
	div.full.f32 %r40, %r41, %r42;
	// end inline asm
	mov.b32 	%f52, %r40;
	.loc	1 126 38
	add.f32 	%f53, %f52, %f1;
	.loc	1 126 28
	rsqrt.approx.ftz.f32 	%f54, %f53;
	.loc	1 120 9
	mul.wide.s32 	%rd15, %r1, 4;
	add.s64 	%rd5, %rd11, %rd15;
	.loc	1 118 19
	mul.lo.s32 	%r97, %r1, %r52;
	.loc	1 118 9
	mul.wide.s32 	%rd16, %r97, 2;
	add.s64 	%rd17, %rd8, %rd16;
	.loc	1 127 16
	setp.eq.s32 	%p24, %r53, 0;
	mov.b32 	%r43, %f54;
	// begin inline asm
	@%p24 st.global.b32 [ %rd5 + 0 ], { %r43 };
	// end inline asm
	.loc	1 128 21
	mul.f32 	%f55, %f54, %f3;
	mul.f32 	%f56, %f54, %f2;
	mul.f32 	%f57, %f54, %f4;
	mul.f32 	%f58, %f54, %f5;
	mul.f32 	%f59, %f54, %f6;
	mul.f32 	%f60, %f54, %f7;
	mul.f32 	%f61, %f54, %f8;
	mul.f32 	%f62, %f54, %f9;
	mul.f32 	%f63, %f54, %f10;
	mul.f32 	%f64, %f54, %f11;
	mul.f32 	%f65, %f54, %f12;
	mul.f32 	%f66, %f54, %f13;
	mul.f32 	%f67, %f54, %f14;
	mul.f32 	%f68, %f54, %f15;
	mul.f32 	%f69, %f54, %f16;
	mul.f32 	%f70, %f54, %f17;
	.loc	1 131 17
	add.s64 	%rd6, %rd17, %rd14;
	add.s64 	%rd7, %rd6, 4096;
	.loc	1 123 64
	mov.b32 	{%rs17, %rs18}, %r18;
	cvt.f32.f16 	%f71, %rs17;
	cvt.f32.f16 	%f72, %rs18;
	.loc	1 129 31
	add.f32 	%f73, %f72, 0f3F800000;
	add.f32 	%f74, %f71, 0f3F800000;
	.loc	1 129 23
	mul.f32 	%f75, %f74, %f56;
	mul.f32 	%f76, %f73, %f55;
	.loc	1 131 30
	cvt.rn.f16.f32 	%rs19, %f76;
	cvt.rn.f16.f32 	%rs20, %f75;
	mov.b32 	%r98, {%rs20, %rs19};
	.loc	1 123 64
	mov.b32 	{%rs21, %rs22}, %r19;
	cvt.f32.f16 	%f77, %rs21;
	cvt.f32.f16 	%f78, %rs22;
	.loc	1 129 31
	add.f32 	%f79, %f78, 0f3F800000;
	add.f32 	%f80, %f77, 0f3F800000;
	.loc	1 129 23
	mul.f32 	%f81, %f80, %f58;
	mul.f32 	%f82, %f79, %f57;
	.loc	1 131 30
	cvt.rn.f16.f32 	%rs23, %f82;
	cvt.rn.f16.f32 	%rs24, %f81;
	mov.b32 	%r99, {%rs24, %rs23};
	.loc	1 123 64
	mov.b32 	{%rs25, %rs26}, %r20;
	cvt.f32.f16 	%f83, %rs25;
	cvt.f32.f16 	%f84, %rs26;
	.loc	1 129 31
	add.f32 	%f85, %f84, 0f3F800000;
	add.f32 	%f86, %f83, 0f3F800000;
	.loc	1 129 23
	mul.f32 	%f87, %f86, %f60;
	mul.f32 	%f88, %f85, %f59;
	.loc	1 131 30
	cvt.rn.f16.f32 	%rs27, %f88;
	cvt.rn.f16.f32 	%rs28, %f87;
	mov.b32 	%r100, {%rs28, %rs27};
	.loc	1 123 64
	mov.b32 	{%rs29, %rs30}, %r21;
	cvt.f32.f16 	%f89, %rs29;
	cvt.f32.f16 	%f90, %rs30;
	.loc	1 129 31
	add.f32 	%f91, %f90, 0f3F800000;
	add.f32 	%f92, %f89, 0f3F800000;
	.loc	1 129 23
	mul.f32 	%f93, %f92, %f62;
	mul.f32 	%f94, %f91, %f61;
	.loc	1 131 30
	cvt.rn.f16.f32 	%rs31, %f94;
	cvt.rn.f16.f32 	%rs32, %f93;
	mov.b32 	%r101, {%rs32, %rs31};
	.loc	1 123 64
	mov.b32 	{%rs33, %rs34}, %r26;
	cvt.f32.f16 	%f95, %rs33;
	cvt.f32.f16 	%f96, %rs34;
	.loc	1 129 31
	add.f32 	%f97, %f96, 0f3F800000;
	add.f32 	%f98, %f95, 0f3F800000;
	.loc	1 129 23
	mul.f32 	%f99, %f98, %f64;
	mul.f32 	%f100, %f97, %f63;
	.loc	1 131 30
	cvt.rn.f16.f32 	%rs35, %f100;
	cvt.rn.f16.f32 	%rs36, %f99;
	mov.b32 	%r102, {%rs36, %rs35};
	.loc	1 123 64
	mov.b32 	{%rs37, %rs38}, %r27;
	cvt.f32.f16 	%f101, %rs37;
	cvt.f32.f16 	%f102, %rs38;
	.loc	1 129 31
	add.f32 	%f103, %f102, 0f3F800000;
	add.f32 	%f104, %f101, 0f3F800000;
	.loc	1 129 23
	mul.f32 	%f105, %f104, %f66;
	mul.f32 	%f106, %f103, %f65;
	.loc	1 131 30
	cvt.rn.f16.f32 	%rs39, %f106;
	cvt.rn.f16.f32 	%rs40, %f105;
	mov.b32 	%r103, {%rs40, %rs39};
	.loc	1 123 64
	mov.b32 	{%rs41, %rs42}, %r28;
	cvt.f32.f16 	%f107, %rs41;
	cvt.f32.f16 	%f108, %rs42;
	.loc	1 129 31
	add.f32 	%f109, %f108, 0f3F800000;
	add.f32 	%f110, %f107, 0f3F800000;
	.loc	1 129 23
	mul.f32 	%f111, %f110, %f68;
	mul.f32 	%f112, %f109, %f67;
	.loc	1 131 30
	cvt.rn.f16.f32 	%rs43, %f112;
	cvt.rn.f16.f32 	%rs44, %f111;
	mov.b32 	%r104, {%rs44, %rs43};
	.loc	1 123 64
	mov.b32 	{%rs45, %rs46}, %r29;
	cvt.f32.f16 	%f113, %rs45;
	cvt.f32.f16 	%f114, %rs46;
	.loc	1 129 31
	add.f32 	%f115, %f114, 0f3F800000;
	add.f32 	%f116, %f113, 0f3F800000;
	.loc	1 129 23
	mul.f32 	%f117, %f116, %f70;
	mul.f32 	%f118, %f115, %f69;
	.loc	1 131 30
	cvt.rn.f16.f32 	%rs47, %f118;
	cvt.rn.f16.f32 	%rs48, %f117;
	mov.b32 	%r105, {%rs48, %rs47};
	// begin inline asm
	@%p1 st.global.v4.b32 [ %rd6 + 0 ], { %r98, %r99, %r100, %r101 };
	// end inline asm
	// begin inline asm
	@%p6 st.global.v4.b32 [ %rd7 + 0 ], { %r102, %r103, %r104, %r105 };
	// end inline asm
	.loc	1 131 4
	ret;
$L__tmp3:
$L__func_end0:

}
	.file	1 "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/unsloth/kernels/rms_layernorm.py"
	.file	2 "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 215
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 114
.b8 109
.b8 115
.b8 95
.b8 108
.b8 97
.b8 121
.b8 101
.b8 114
.b8 110
.b8 111
.b8 114
.b8 109
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 122
.b8 101
.b8 117
.b8 115
.b8 47
.b8 109
.b8 105
.b8 110
.b8 105
.b8 99
.b8 111
.b8 110
.b8 100
.b8 97
.b8 51
.b8 47
.b8 101
.b8 110
.b8 118
.b8 115
.b8 47
.b8 99
.b8 108
.b8 111
.b8 117
.b8 100
.b8 115
.b8 112
.b8 97
.b8 99
.b8 101
.b8 47
.b8 108
.b8 105
.b8 98
.b8 47
.b8 112
.b8 121
.b8 116
.b8 104
.b8 111
.b8 110
.b8 51
.b8 46
.b8 49
.b8 49
.b8 47
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 47
.b8 117
.b8 110
.b8 115
.b8 108
.b8 111
.b8 116
.b8 104
.b8 47
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 115
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 95
.b8 103
.b8 101
.b8 109
.b8 109
.b8 97
.b8 95
.b8 114
.b8 109
.b8 115
.b8 95
.b8 108
.b8 97
.b8 121
.b8 101
.b8 114
.b8 110
.b8 111
.b8 114
.b8 109
.b8 95
.b8 102
.b8 111
.b8 114
.b8 119
.b8 97
.b8 114
.b8 100
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 141
.b8 4
.b32 141
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 125
.b8 21
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
